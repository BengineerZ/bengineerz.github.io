---
title: "Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation"
authors: ["Andr√© Schakkal", "<strong>Ben Zandonati</strong>", "Zhutian Yang", "Navid Azizan"]
venue: "Robotics Science and Systems (RSS) Workshop on Robot Planning in the Era of Foundation Models"
date: 2025-06-28
image: "/papers/hierarchical_vision_language_planning.mp4"
tldr: "This work introduces a hierarchical and modular vision-language architecture for multi-step humanoid manipulation tasks. Specifically, we leverage vision-language models (VLMs) for high-level planning and action-chunking transformer (ACT) and whole-body RL for mid and low-level control."
arxivUrl: "https://arxiv.org/abs/2506.22827"
projectUrl: "https://vlp-humanoid.github.io/"
---

n/a
